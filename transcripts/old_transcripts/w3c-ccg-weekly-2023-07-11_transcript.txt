 Hi everyone, so welcome to this week's W3CCG. This week we have Hank and Carson. I think Carson will let them introduce themselves a little bit later. We have been actually presenting the Seaboard, Consize Binary Object Representation today. But before we get to the main agenda, I just want to do a couple of admin things. First of all, just a quick code of ethics and professional conduct reminder. More or less, just make sure that we remain respectful and acknowledge each other's perspective at all times. Now, quick IP note, anyone can participate in these calls. However, all successive contributions to any CCCG work items must be member of the CCCG with full IPR agreement signed. So make sure you have the W3CC account and sign the W3CC Community Contribution License Agreement. If you have any questions or encountering any problems, just let any of the co-chairs know. A quick IP note, we use GGG Chat to keep speakers during the call. So just type in QPlus to add yourself to the Q, Q- to remove it. All the meetings, minutes and note recordings will be published within the next few days. All right, any introduction or reintroductions? Okay, I'm going to cue a Carson. Do you mind introducing yourself a little bit? Yeah, hi, I'm Carson Bowman. I'm a professor at All Visited Primm in Germany. And I have been trying to bring the internet to strange places for more than three decades now. And recently, I have been working on IoT related issues. So I'm very interested in doing things with little resource usage, which of course is very modern now in the era of green IT. But I really needed to be able to put it on a better operator sensor or something like that. And I happened to do my PhD thesis about something which probably today would be called Exima Technologies. The tag that wasn't invented yet. So I have a certain history in doing data representation. And I think that we'll try and through the next minutes. Wait, thank you, Carson. I think I had little technical difficulties. So let me restart the subscriptions. Hold on a second. Thank you. And also Hank, do you mind introducing yourself a little bit to the audience? Yeah, hi, Harrison. This is Hank. Is it my audible? Yes. Wonderful. Okay, yeah, it's a little bit tricky sometimes. But this works. Okay, yeah, hi, I'm Hank. I'm also located in Germany. I'm working for two research institutes at the moment. One of them is for secure information technology. And the other one is for material science, engineering and research. So that is a weird combination. My day job is material science at the moment. But I've been doing as not as long as Carson, of course. But to some extent, some standardization in the trusted computing group and the internet engineering task force. Why is that? We wanted to have trustworthy assertions about rather a lot of interesting domains. BAB, the Department of Defense as a military set. They have assets or other things that we want to believe about assertions. And that's where I think the trusted computing group was great. They invented the term attestation. Recently, NIST redefined that into being an endorsement. But sure, let's do that. And they are not so good at building protocols sometimes. So the IETF was a good place to try to implement or specify a protocol for remote attestation that's a bit different from challenge response. And then the realization came, but well, nobody knows how this works. And also, everybody wants to use SIBO for this. That's great. TPMs do not fully SIBO outputs. And that is an interesting conundrum. So how do you combine something with the powerfulness and the usefulness of SIBO in such while you have to deal with legacy native formats. You can't get rid of because they are soldered into your IoT device for the next 25 years. And yeah, this is why I'm basically also here in Berkowitz-Karson a lot because I'm trying to take a sub-section of Karson's problem domain and Karson tries to take a subset of my problem domain and really makes a lot of sense to do this together. Thank you, Hank. Any other introductions and reintroductions? Feel free to unmute yourself. Okay. What about announcements and reminders? Anyone have any announcements or reminders? Korea, please. Just a reminder that the Internet Identity Workshop in Mountain View, California is coming up October 10 to 12. And so it's sort of early this year. So and I'll also say we're really committed to accessibility. So if you want to be there, we want you to be there. So reach out to us. We can work with you. And yeah, I think it's going to be a great number 37. Sounds good. Thank you, Korea. At Econo. Econo, you have the floor. You want to talk about the reboot weather trust? All right. We can come back to you at a later time. Any other announcements or reminders? All right. Any updates on the work items? Please, Monia. Yeah, I'll just mention, since Oliver isn't here, there was a work item that was put out there around confidence methods around verifiable credentials. So this is a capability to allow an issuer of a verifiable credential to convey how they might verify that the entity or individual presenting that credential is the same person that picked up the credential. So one way that you can establish confidence is the person uses the same public key pairs of a proof of possession of a key is one mechanism, but there are other mechanisms available such as check their driver's license. That could be one confidence method. It's been floating out there for a while. The issue is that I don't think Spruce has the time to push that specification forward in order to we and I think there's just an open call for anyone that's interested in that feature if folks want to take that spec over. There's just an open call for someone picking up that spec. So that's the first one. The second item had to do with the verifiable issuers and verifiable verifier lists. So this work item is about how do you know if an issuer should be trusted? So people call these like trust lists or a trust frameworks, but the idea here is that for example, if you get a digital driver's license, how do you know that the issuer is allowed to issue that driver's license? Or the flip side is if someone asks you for your driver's license, how do you know if they're supposed to be able to ask for it? Like are they an authority of some kind? Or do you need to give a special exception to hand that license over? That group had their first meeting last week. I believe that group's going to meet every other week. So this is the verifiable issuers and verifiers task force that's operating. I think they'll be setting up agendas every two weeks. And this is work that came out of the rebooting the Web of Trust, blast rebooting the Web of Trust around knowing who to trust, who's issuing credentials and who's verifying credentials. That's it. Great. Thanks. Thanks, Mani. Any other updates on the work items? All right. Let's get to this week's main agenda. So over the past few months, I think there's several threats and conversations regarding C-board concise binary object representation. And today we have two of the most top experts on this topic on the call. So Karsten and Hank, and we're very, very lucky and honored to have them here to actually help present and answer any questions on C-board. So Karsten, Hank, the floors are yours. Hi. Starting slant share hides the onion button. That's great user interface design. Okay. I want to take about half an hour slightly more than half an hour to introduce two topics here. C-board, the concise binary object representation, and C-board is the concise data definition language. And so first of all, to explain where we are in the grand scheme of things, this is about representation formats. So in particular about serialization, so how do the bytes look like when you send data somewhere when you store data somewhere, but also about data models, because if these bytes don't mean something in some data model, then you haven't really achieved very much. What I'm not going to talk about is information models, so that the higher level views of the meaning of information that of course also important, but not the subject of today's talk. And I think the summary of today's talk, if you came from XML, then maybe you have heard about something called XE, which was a binary form of XML. Unfortunately, that came out when XML was in the process of being replaced by JSON for many applications, so XE didn't get quite as popular as it maybe deserved. But there has been a long trend to actually get from character-based, text-based formats to binary formats in many places. So HTTP 1.1 has been replaced by HTTP2 and so on. And this is a trend that we are seeing in representation formats as well. And up to 10 years ago, we really didn't have anything for the bottom right corner of this chart. And that's why Cebo was developed. I can actually put this into even more context starting 40 years ago with HTML1 or X409 as it was originally called, which was the first representation format that was designed to enable interchange. I think there was a question. Yes, Claire, do you have a question? I think that she removed herself on the queue. So please, continue the text. Yes, and one has been very popular and still is popular in another of places. And the next rows here all have been very popular and still are popular for certain applications. So we are seeing this about once a decade replacement of data representation formats, the popular ones going on. So we had XML in the mid 90s. We had JSON in the mid 2000s and we had Cebo in 2013. So these representation formats differ in how they treat data definition. ASN1 actually is a data definition format that just came with one or two encodings. So the encoding was more or less an afterthought. While the other, the more recent ones usually put a lot of focus on the encoding mechanism and enable the use of different data definition mechanisms with that encoding. So there's one XML, but you can run XML with GTD, with WS3 schema with RelaxNG and so on. That appears to be a slightly more healthy way to do this. So this is the background and the recent work in this area has been documented by the IGF. So for instance, JSON is documented in RFC 82.59 or standard 90, as we call it in the IGF. And what JSON did essentially was go away from a really complicated generic data model that had elements in the attributes and processing instructions and lots of stuff. And we use this to generic data model that the programmer would want to use. So we have false true null. We have number strings array, something called objects which the computer science person would call maps and so on. And with this generic data model comes a text based serialization. And that was taken from JavaScript, which was a great way to in discussion. Otherwise, you get in this big ring about the format. And that the use of the notation from JavaScript didn't mean that originally JSON was was tightly married to the JavaScript data model. But in practice, this is what has happened. So we now have some limitations from the JavaScript data model in particular from the number system going into rules for interoperable use of JSON, which are documented in RFC 7193. JSON also has brought a style with it. So we will use maps as structures where we put data under names, map keys. And we have a form of extensibility there, which is really important for JSON based interchange. So you can just add a key member name to your object and put in more information. So you get backward compatibility and even maybe some forward compatibility if implementations can simply ignore the additional information. So this is really a major step. And I don't think this is going any way. So the JSON data model is really, really important innovation. It's also very limited. Of course, you can always find things that don't fit there very well, but it's really a good basis. There are various data definition languages. In the early days of JSON, there was an aversion to actually two things like validation. But it has turned out that it's at least sometimes useful to have validation mechanism. So we have JSON schema.org. And we have CDDA, which we will talk about briefly. And we have Yang in the ITF, which is another data modeling language that is particularly useful for describing management information basis at rest. So that's the background. And when we started to design Cball, we decided that the JSON data model really is very successful. And it has a lot of decisions done right. There was a number of binary JSONs coming up at the time. And one of those was called message pegs. So if you will remember that. And the idea was if we want to have a binary representation, which is much more useful for a small IoT device than a text-based representation, let's stick with the JSON data model and add what we need. And what we needed is byte string data. So binary data like certificates and sensor data and so on. And so that is in addition to the data model. And we also moved the serialization to binary. So we no longer have text-based representation. And we separated the integers from the floats, which we originally hadn't really thought about very much, but that has turned out to be very useful for performance critical applications in particular in the IoT space. And then we have records or vectors like the JSON arrays and structs or tables like the JSON objects, which we call with a computer science term map. The actual representation format, I'm not going to talk about that. But it's a classical type length value mechanism. But we have learned about problems that type length value mechanisms can have, including ASN1, basic encoding routes. So we use byte lengths only at the leaf level, lowest or highest, depending on where the roots of your freezer level and count items otherwise. So there is no way for an attacker to put a wedge into your system by using inconsistent byte lengths. So C was designed for IoT use, but it also was designed in a generally applicable way. So it's now a little more than 100 ITF specs as a normative representation. And it's out there in all kinds of places, mobile driving, license and cover test certificates and FIDO tool and lots of syntax stuff, global platform. So here's a little table in the teletype RFC style we were using in the IGF, which just shows a SIBO representation of a particular array of the number one and another array, which has the numbers two and three. And you can see that other binary JSONs were a little bit on the verbose side and also on the really hard to process side. But what do you do with all this data and CBO was trying to be concise, which is exactly the first letter in the word SIBO and the P, of course, is binary. So that's all I want to say about, not quite all, I want to say about the representation format. As I mentioned, TLV mechanisms are a hacker's playground and we have learned from that. We also have learned that using GSAV numbers is, creates interesting CVs that nobody would have thought about before that. So security, the ability to write secure software with SIBO has been on the top of the list all the time. And actually, SIBO also is used a lot for security. We increasingly need objects that have some some signing or encryption or both applied to it. And of course, you all know about HOSI and JSON web tokens and so on. And COSI is the way we can do this with SIBO. And this became pretty obvious right when SIBO was standardized. So we started a working group in 2015 and we now have a full internet standard, RC-9052, which defines the COSI performant and a few RCs that come with that the algorithm registry, the hash mechanism registry advanced counter signatures and so on. Again, initially, this was used for internet of things or web of things. And it's now used in many other environments, including some 50 ITF specs. And of course, we didn't reinvent the wheel here. So this all looks very familiar if you come from ASN1, CMS or XMLD. So we see the differences that we picked up the idea of HOSI to actually keep the data that are being designed or encrypted in the interchange. So we don't have issues of canonicalization that other such mechanisms often have. But maybe that wouldn't have been necessary because SIBO comes with a pretty good canonicalization except that we call the deterministic encoding. So the fundamental problem is that in most representation formats, one data model instance can have multiple civilizations. So that is not good if you want to use them as signing inputs. So ASN1 had something called deterministic encoding rules that allowed you to nail down how exactly a really fine piece of data model look like on the representation. And the same thing is part of the SIBO standard. And it's actually quite easy to do because the data model already is deterministic encoding friendly. We have a preferred encoding and only the one thing that needs to be added that extra works or that's why we don't always use it. You have to order the keys in a map in a deterministic way to get fully deterministic encoding. There are also some application choices. For instance, if the application gives you a date or date time in this case, there may be choices of representing the same time in the generic data model. And you also have to think about deterministic encoding at the application level. So for instance, in this case, you would always use the Z form and not the plus 0 0 0 0 form. So we need to have some application thinking about deterministic encoding. There's actually one interesting proposal out there that is being discussed, which concerns number types. So the DCBO proposes to map all application number types on top of each other, which means that you convert floating point values to integer values if they are integral numbers, if they are round numbers. Okay, but I didn't really want to dwell on the deterministic encoding too much. I want to point to a playground, which is quite useful to get some experience with CBO. CBO, of course, is binary encoding. So you need tools to work with it. I would argue that you need tools to work with any text encoding as well. You cannot read the JSON data that go over the network these days. Anyway, so this is a tool that converts between CBO, which is shown in hex on the right side. And something we call diagnostic notation. So when you look at this, you will notice this is all of those, but not entirely unlike JSON. And that was the idea. We just took the JSON representation format and everything that stays within the JSON data model also is represented in original JSON formats. And if we have other things like hex data or binary data, or tags, which are an extension mechanism I didn't really talk about yet, then we just extend the syntax. So the diagnostic notation you see on the left side, which almost looks like JSON, that is not really meant for interchange, maybe for interchange between tools. But the actual interchange of data is supposed to happen with the hex format on the right hand side, which is way more compact. Okay, so end of advertisement for the CBO.ME side. And I want to finish the CBO segment with a quick outlook on what we are doing right now. The CBO working group in the ITF has nine RFCs at this point in time, which define both CBO and CBO. And the CDD data definition language. And what we are working on right now, but what we are almost you almost have completed right now is the CBO PACT format, which allows you to still interchange CBO, but increase the conciseness without decreasing the efficiency. So you don't have to run a decompression mechanism before actually working with the data. You can work with the data right away. So that's pretty interesting. And we also have various common application formats, which we usually define as CBO tags, which is an extension mechanism. And CBO came with an original date and time format. But that was kind of basic. Okay, for batteries included, but there are people who have way more detailed requirements on time stamps, like accuracy guarantees and then clock sources and so on. And there is no attack definition that does that. And there is also a document defining CRIs, which are a structured representation of URIs. So URIs are text strings and CRIs present the actual content of that UI in a structured way, which can make it easier for for an application to work with it. The other thing that we are working on and that will probably not be done by the end of the summer, but still within this year, is something we call CDDL 2.0, which has additional support for larger data models. But I'll talk about that next. So this is the end of the CBO part of my presentation. Do we have any questions before I go into a CDDL? Monio. Hey, Carson, what wonderful presentation. Always really interesting to kind of hear from the people that created these languages, what went into them. I'm interested more about kind of the creation of CBO. You mentioned that there were a number of other packed binary formats that I would presume CBO was kind of competing against with respect to attention and which one of these things is going to work. And you know, Message Pack was backed by some large organizations. How did that kind of interplay work out in the group? Did you have to adjust the initial CBO design? Like you mentioned, there was a split between integers and double position numbers. What was the environment like during the creation of CBO? Was there a lot of collaboration between all the different packed binary formats? Or was everyone kind of in their own camp trying to make their packed binary format like Message Pack or Bese and or Win? Any thoughts on kind of how that played out in the working group and outside? Sure. Let me do one terminology thing first. So I'd like to call these binary formats or in the code case of CBO, concise binary formats. Some of the other binary formats are not concise. Packed goes beyond that by actually removing redundancies from the data model. And that's something that some data formats actually do, but that was not in the set of things we wanted to do in 2013. So the way we got into CBO was more or less accidental because we looked at a number of representation formats and very quickly decided that Message Pack was the clear winner of all of them. It had some weird things because it has grown over time, but it does a lot of things right. So we tried to discuss with the Message Pack people whether there would be room for standardizing Message Pack or maybe a Message Pack variant that fulfilled the requirements we had. But it turned out that the Message Pack community at the time was not interested in having their ecosystem being disturbed by a standardization effort. And yeah, clearly if you have applications that use this and everything works and so on, why would you want to have standardization around? So we at some point we decided, okay, let's take Message Pack, remove some of the weirdness, make this way more regular in a way that you can implement it in some 20, 25 lines of code. And let's call that CBO. So that's the background. And Message Pack is being used today. So I don't think the air consideration we have our own digital ecosystem, which is not disturbed by other activities that still valid the thing to work. Let's talk about Bson quickly. Bson is a very interesting format because it's actually not an interchange format. It was designed to describe database data address. So you could patch these data in place when some values in this data changed. So they had to provide a lot of allowance for the patching value to be bigger than what they actually needed to represent in the first place. And that's why Bson is so ridiculously larger. You see all these zero bytes there. And the zero bytes are there because they leave space for patching things in place. So Bson really is not an interchange format. It's a data address format. But of course you can use it for interchanges. I think that the main reason that Bson got as much attention as it got at the time was that people were asked by the managers, isn't there a binary JSON? And then there was Google and the first thing that they thought was Bson. And it's really bad. But we just for interchange purposes. So that was this of interest to us. Is your question answered? Yes, thank you. Thank you. The other questions? So I see that there are some 20 minutes, not all of which I probably can use. So I run through the rest of the slides a little bit faster. But let me quickly talk about the concise data definition language. So that happened quite a bit later. So we had C4 in use and we were quite happy. But actually you want to have a data definition language at some point. If only just to be able to talk about what you are interchanging. And it's not sufficient to say we are using JSON as our interface format or we are using C4 as our interface format. That's about as useful as saying I'm using ASCII as my programming language. You really have to say what those structures are going to be. You have to describe the specific data model that the application is going to use. It's not going to use JSON's generic data model. It's going to use a specific one through this specific to this application. And you can do this in English, of course, as well as all protocol specifications. Or you can do this in a machine readable and verifiable way. And that's really what we were interested. And it turns out you can do this for both JSON and Cibola at the same time. Because the Cibola generic data model is just a simple set of the JSON one. So everything you can describe in the JSON document is also valid Cibola. So we can use the same language for both. So when we started to think about the CDD, we had two important influences. One is the ITF has been using Baccus's, now our formats since 1977. So way before other people started thinking about maybe standardizing an EVNF format, something like that. And that was finally written up in RFC 5234, the augmented EVNF that we use in the ITF for all text-based protocols. And we wanted to make it really easy for people who come from text-based protocols to transition to CDD. So we took the overall syntax of AVNF, which, yeah, you notice that it's 46 years old. Computers were a bit different at the time. Tomnits were just learning to do lowercase and so on. So in some places it's a little bit weird, but generally, AVNF is a nice cultural basis to define a language on top of. The other influence was Relax and G, which was designed in the W3C. After W3C schema had provided a way to define XML data. And people were a little bit unhappy and Relax and G was trying to reduce this in a simplified form. And Relax and G was a major step by saying, okay, we don't have to be homo-connect. We can actually use a human readable format for this. And Relax and G compact was that human readable form. So we learned from both AVNF and Relax and G compact. We had a stable start in 2016 that finally was published in 2019. And as I mentioned before, we are working on putting some additional things in the backward and forward compatible way. So let me show you a piece of CDDL. At the time we were looking at a specification, RFC 7071, which defined the reputation object. What there is in detail is not relevant here. But on the right hand side of the slide, you can see the formal definition that was essentially done in a stylized form of English to describe what a reputation object is. And on the left hand side, on the left hand side, you can see how this looks in CDDL. So a reputation object is a JSON object that has an application member and a reputable member. And the reputable member is an array of zero or more reputable ones. And reputable objects again with a ray-ton assertion a ray-ton text, a ray-ting floating point value, and optionally a very use confidence number rating, and so on. And finally, you catch all four extensibility. So this is way more useful. And also when I did the translation from the text form on the right hand side to the formal different vision on the left hand side, there were several questions I had to decide because the English language form of course wasn't well-defined. There were things in there which were open. So that was kind of what was motivating us. And I'm going to run really quickly through the next ten slides. So BNF has productions and CDL also has productions, but these productions aren't about text strings. They are about trees and the data model. And these trees are described in two ways using data items themselves, types, or sequences of key value pairs, which are groups. The top level production in CDL is always a type, but can use groups internally. So types are U-inch text bytes, time. You can have choices, one true bar. You can have various forms of literals. And yeah, the choice is a pretty widely used construct here. There are also constants and ranges and so on, regular expressions. I'm not going to go into that. Groups are then used to create sequence. So for instance, if you have an RGB value, that might be a red and a green and a blue and all three bytes. You answer integers of size one byte. And you can also use current indicators. So star means zero or more. Three star, three means exactly three, and so on. And you can combine this. So you can, for instance, say a message is an array with a message type and zero or more parameters. So these groups can then enter and be named. So in this case, the optional parameters actually is zero or more items that go into this array. So what we have been seeing here, essentially, were groups. And this is kind of the basic form to write this. You see a little bit of noise here. Let lots of code marks and so on. And one little feature of CDL is that it provides some noise reduction. So if you're using normal typical JSON style, then you don't have to write up this codes and commas and all that. So yeah, occurrence, we learned a little bit from EBNF here. I'm not going to go into that detail. We have choices on the group level as well, which can be used for some basic co-occurrence constraints. So in summary, groups encapsulate sequence or things that come together in maps. Maps, of course, don't have a sequence. And are useful to get a little bit more information into the data definition. So at some point, specifications get larger. And then we have certain things to help with that. So we have ways to build up productions incrementally. We have generics. We have things like unwrapping. So we can take a ready-made type map array or tag and use the part. And we have so-called control operators, which do more complicated things that don't quite fit in the general, make us know a form mechanism that CDL is based on. So finally, if you want to work with CDL, you often want to check your CDL specification for syntax problems. You want to validate the data item against the CDL spec or you want to generate random valid instances from a CDL spec, which is probably the most useful thing during your design phase. You will be surprised what your data definition actually allows. And finally, core generation is also useful, even though we haven't been emphasizing that. So there is a tool that actually, much of the tools now, I need to update this slide. So there is a CDL tool, and there is one for doing CDL 2.0 things called CDDC. But I'm running out of time. So we have some time for more questions. I'm going to stop here. Bonnie. On CDDL, thanks Harrison. On CDDL usage, it feels like it's multipurpose, of course. One of the usages is to, I guess, replace ABNF syntax or just BNF syntax in IETF specs that work with C-BOR. So the first question is just confirmation that that's one of the primary use cases there. The other use case is potentially as a replacement for JSON schema. Is that a design goal or is that just, you know, it only does a subset of JSON schema? What are the thoughts as far as verifying objects and life systems using CDDL libraries? Well, a few slides earlier, I said that I think it's a good thing to actually have multiple data definition languages. I don't think ABNF is really very useful to describe C-BOR. It's not even useful to describe JSON because you would be forced to encode all the details of how JSON encodes certain things. Think about surrogate pairs into each data type that you are building up. That doesn't make sense. So you wouldn't use ABNF to describe JSON. Although I know it has been done, but it's generally recognized as a mistake to do so. So ABNF and CDDL solve different problems. One is about text and the other one is about the trees of data. No, we cannot ignore surrogates because we are based on JSON here and JSON uses the surrogates to represent code points above 10,000 texts. So yeah, we have to live with that. So the other question about the other mechanism for JSON schema is fine for its area of application. So if you have tool-based development model where people don't really look at the schemas as much as they use tools to operate on them, JSON schema is fine. It's a bit idiosyncratic in some places. So I have written converters between JSON schema and CDDL. It's hard to get these right in the general case. So that's a piece of work that's still ahead of us. But again, I think it's good to be polyglot on the data definition side. CDDL is good if you really want to understand what's going on. So typically when I get a JSON schema, I translate that into CDDL and then I see surprising things. I didn't see of JSON syntax before that. But there is, for instance, also Yang, which is another format that can be used to describe both JSON and SIBO data. And that's very popular in the network management community and related areas in the IGF because it's really good for describing data at rest and describing operations on data at rest and not just the data themselves. So that's another data definition language that we would be embracing. And I think it's really important for us to emphasize that that SIBO is not married to CDDL and there even have been some noises recently that JSON schema probably should add really a few things that are needed to make it work for SIBO as well. So that's not much that needs to be done. I hope that answers your question. It did. Thank you. Hank. Yeah, I'm just adding one thing that I feel surprisingly is missing concursant supply is that it's to convey what you're doing from human to human. The same way it is to enable very lightweight validation of structure. So it does both. The 3 grama aspect provides lightweight burden on constraint devices and the noiseless compact representation provides light weight on the human. You can really read CDDL and you can write it fluently. I had this exercise where someone was explaining what protocol they want to have, what goes where we had a sketch for it and everybody was like, okay, when can we have the specification for this protocol? I wrote it just while you were talking about it. Here it is. Here you go. And they're expecting weeks and it took me the same time they were talking about it. So you can, if you are familiar with it, you can write fluently by your designing protocol messages. And I think that's one big enabler of CDDL. I think that was a little bit lost in the context. Thanks, Hank. And David, you have a question? Yeah, I've enjoyed the presentation so far. With CDDL having basically like 65-bit integers with tagsline for new extensions for data types such as times and durations. I was wondering whether usage has been more along the lines of defining messages pretty concretely to trade between like parties as concise language or more as a object model for having data and data archival with more of like a, say, a DOM type mechanism for representing the C-More. Yeah, we call that the data address was a data in flight. And CDDL certainly was designed with explicit objective to be able to describe messages for interchanges, so data in flight. And one would have to think about how descriptions of data at rest relate to these descriptions. So people who write data at rest formats often try to do some, not so successful job at defining data in flight as well and vice versa. And I don't want to be that guy. But generally the data model can be used to describe an information model in very many cases that also can be used for data at rest. It's just not maybe not the foremost consideration for us. I guess to expand upon that, you have back in the XML day, so you had kind of the document crowd that considered that SVG and HTML that you'd have specialized tools for particular document formats. And then you had the data crowd which were geared more towards tools that were universal across everything you could possibly represent with XML. And sometimes there was a bit of an impedance mismatch between the two. And it wound up making it more confusing for people who wanted to use XML because different tools were optimized for different problems, which is kind of why we had XML schema and the MG stuff, the LexNG at the same time is because XML always served those two markets. I'm just wondering if you saw C-bar being more used for like very specific schema data format or more free form extensible data formats? Well, it can do both. And I think the important thing if you do larger things is being able to do composition. So you have this one piece of Citi there that exactly defines how to stay in your example. SVG looks like, and you have another one that defines HTML. And now you have to describe how the two fit together. And that's exactly the objective of Citi 2.02 to make that kind of composition easier. All right. Any one last question or comment? All right, Carson or Hank, do you have any like conclusions, any last conclusions for us? Well, just thank you for providing the opportunity to tell a little bit about what we are working on. And of course, we would be very interested in getting feedback on the current working items, work items, work items, work, working, work, work. I think I'll send her on to that working group after the meeting. Sounds good. Well, thank you. Thank you, Carson. Thank you, Hank, for hopping on the call. And then actually, C-bar again has been brought many, many different times. So thank you for spending time to kind of answer, and not just doing this presentation to answer the questions for us. If you don't mind sharing these slides, I can follow with you offline, and then share it to the public list. All right, thank you. Thanks, Carson. And thanks, Hank. And this concludes this week's W3CCCG meeting. Thanks, guys. Thank you. Bye-bye. Bye. Bye. Bye-bye.